{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:\\\\Users\\\\mesho\\\\OneDrive\\\\Desktop\\\\SentiSum\\\\sentisum-assessment-dataset.csv',header=None)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.dropna(axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### The top most entry of the data\n",
    "df.loc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### the top 5 entries of the dataset \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include=[object])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords  #stopwords\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "stop_words=set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\") #snowball stemmer\n",
    "original_words = ['alumnus','universal', 'waited', 'Flying', 'caring', 'flies', 'dies', 'agreed', 'owned', \n",
    "           'humbled', 'sized','meeting', 'state', 'siezing', 'itemization','sensational', \n",
    "           'traditionally', 'referencing', 'colonizer','plotted','providing'] \n",
    "plural = [stemmer.stem(plural) for plural in original_words] #Stemmed into plural form\n",
    "\n",
    "pd.DataFrame(data={'original word':original_words, 'stemmed':plural})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(WordNetLemmatizer().lemmatize('working', pos = 'v')) \n",
    "# past tense to present tense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "      text = re.sub(\"@[A-Za-z0-9]+\", '',text)\n",
    "      le=WordNetLemmatizer()\n",
    "      word_tokens=word_tokenize(text)\n",
    "      tokens=[le.lemmatize(w) for w in word_tokens if w not in stop_words and len(w)>3]\n",
    "      cleaned_text=\" \".join(tokens)\n",
    "      return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned_text']=df[0].apply(clean_text)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect =TfidfVectorizer(stop_words=stop_words,max_features=1000)\n",
    "vect_text=vect.fit_transform(df['cleaned_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda_model=LatentDirichletAllocation(n_components=12,\n",
    "learning_method='online',random_state=42,max_iter=1) \n",
    "lda_top=lda_model.fit_transform(vect_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = vect.get_feature_names()\n",
    "for i, comp in enumerate(lda_model.components_):\n",
    "     vocab_comp = zip(vocab, comp)\n",
    "     sorted_words = sorted(vocab_comp, key= lambda x:x[1], reverse=True)[:12]\n",
    "     print(\"Topic \" +str(i)+\": \")\n",
    "     for t in sorted_words:\n",
    "            print(t[0],end=\" \") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,topic in enumerate(lda_top[0:]):\n",
    "  print(\"Document \",i,\": \",topic*100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_top=lda_top*100\n",
    "lda_top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_top = lda_top.astype(int)\n",
    "lda_top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxvalues = np.amax(lda_top, axis=1)\n",
    "maxvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(maxvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexs = np.argmax(lda_top, axis=1)\n",
    "indexs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see here these indexs are clusters which they belong to as these values are the probabilities where they actually belong to. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next goal would be to quantify the clusters (if many clusters have nearly same score then it would be difficult to handle many categorical values at the same time). Though this approach may/may not be the correct one but to seems appropriate as we can merely not choose sole on the basis of high probability.\n",
    "\n",
    "So we'll be using some assumptions and use of bias here to annoatate the dataset and then train subtask b."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's first check our dataset for assumption 1 that is values > 0.5 or 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greater_than_50 = (maxvalues > 50)\n",
    "greater_than_50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_50 = [i for i, val in enumerate(greater_than_50) if not val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(res_50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as per our dataset we can clearly say that close to 5,047 samples have values > 50 (percentage) which means our model was smart enough to classify it on the basis of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#less than 50 and greater than 40\n",
    "l_50_g_40 = (maxvalues > 40) & (maxvalues < 50)\n",
    "l_50_g_40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resl_50_g_40 = [i for i, val in enumerate(l_50_g_40) if not val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(resl_50_g_40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as per our dataset we can clearly say that close to 2,161 samples have values greater than 40 and less than 50 (percentage) which means our model was smart enough to classify it on the basis of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#less than 40 and greater than 30\n",
    "l_40_g_30 = (maxvalues > 30) & (maxvalues < 40)\n",
    "l_40_g_30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resl_40_g_30 = [i for i, val in enumerate(l_40_g_30) if not val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(resl_40_g_30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as per our dataset we can clearly say that close to 1823 samples have values greater than 30 and less than 40 (percentage) which means our model was smart enough to classify it on the basis of the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we'll add the indexes that we calculated as theose were the clusters only, so adding it to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['index'] = indexs\n",
    "df['high_val'] = maxvalues\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data anootation part done we'll start with training the supervised machine learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['cleaned_text'], df.index, test_size=0.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Buidlding Count Vectorizer to convert the Messsage to Vectors\n",
    "vect_df = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_text = vect_df.fit_transform(X_train)\n",
    "X_train_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#Algorithms\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_params = {\n",
    "#     'svm': {\n",
    "#         'model': svm.SVC(gamma='auto'),\n",
    "#         'params' : {\n",
    "#             'C': [1,10,20],\n",
    "#             'kernel': ['rbf','linear']\n",
    "#         }  \n",
    "#     },\n",
    "#     'random_forest': {\n",
    "#         'model': RandomForestClassifier(),\n",
    "#         'params' : {\n",
    "#             'n_estimators': [1,5,10]\n",
    "#         }\n",
    "#     },\n",
    "#     'logistic_regression' : {\n",
    "#         'model': LogisticRegression(solver='liblinear',multi_class='auto'),\n",
    "#         'params': {\n",
    "#             'C': [1,5,10,15]\n",
    "#         }\n",
    "#     },\n",
    "#     'decision_tree': {\n",
    "#         'model': DecisionTreeClassifier(),\n",
    "#         'params' : {\n",
    "#             'criterion': ['gini', 'entropy'],\n",
    "#             'splitter': ['best','random']\n",
    "#         }  \n",
    "#     },\n",
    "#     'knn': {\n",
    "#         'model': KNeighborsClassifier(),\n",
    "#         'params' : {\n",
    "#             'n_neighbors': [5,7,9,11],\n",
    "#             'algorithm' : ['ball_tree', 'kd_tree', 'brute']\n",
    "#         }\n",
    "#     },\n",
    "#     'naive_bayes' : {\n",
    "#         'model': GaussianNB(),\n",
    "#         'params': {\n",
    "#         }\n",
    "#     }\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores = []\n",
    "\n",
    "# for model_name, mp in model_params.items():\n",
    "#     clf =  GridSearchCV(mp['model'], mp['params'], cv=2, return_train_score=False)\n",
    "#     clf.fit(X_train_text, df.index)\n",
    "#     scores.append({\n",
    "#         'model': model_name,\n",
    "#         'best_score': clf.best_score_,\n",
    "#         'best_params': clf.best_params_\n",
    "#     })\n",
    "    \n",
    "# vals = pd.DataFrame(scores,columns=['model','best_score','best_params'])\n",
    "# vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb = MultinomialNB()\n",
    "lr = LogisticRegression()\n",
    "dt = DecisionTreeClassifier()\n",
    "rfc = RandomForestClassifier()\n",
    "svm = SVC()\n",
    "knn = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.pipeline import Pipeline\n",
    "# clf = Pipeline([\n",
    "#     ('vectorizer', TfidfVectorizer()),\n",
    "#     ('nb', MultinomialNB()),\n",
    "#     ('lr', LogisticRegression()),\n",
    "#     ('dt', DecisionTreeClassifier()),\n",
    "#     ('rfc',RandomForestClassifier()),\n",
    "#     ('svm',SVC()),\n",
    "#     ('knn',KNeighborsClassifier())\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb.fit(X_train_text,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(X_train_text,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.fit(X_train_text,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc.fit(X_train_text,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm.fit(X_train_text,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.fit(X_train_text,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb.score(X_train_text,y_train)\n",
    "lr.score(X_train_text,y_train)\n",
    "dt.score(X_train_text,y_train)\n",
    "rfc.score(X_train_text,y_train)\n",
    "svm.score(X_train_text,y_train)\n",
    "knn.score(X_train_text,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emails = ['A perfectly easy way to order tyres online. Just enter your car registration number, check the recommended tyres are correct and select the tyres you want. Select the best time and venue for the fitting and pay online. The whole process is easy and the best value for money!'\n",
    "]\n",
    "emails_count = vect_df.transform(emails)\n",
    "model.predict(emails_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_train_text,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
